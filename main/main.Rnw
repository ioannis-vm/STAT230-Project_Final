\documentclass[11pt]{article}

\usepackage{comment,fancyhdr,amsthm,amsmath,hyperref,comment,hyperref,enumerate,float,datetime, microtype, setspace}
\usepackage[headheight=14.5pt, width=6.2in,top=1.1in,bottom=1.1in,bindingoffset=3mm]{geometry} % margins
\usepackage[backend=biber,style=authoryear,sorting=none, maxcitenames=2]{biblatex}

\pagestyle{fancy}  % add headers and footers

\renewcommand{\baselinestretch}{1.50}  %  set default line spacing
\newdateformat{monthyeardate}{\monthname[\THEMONTH], \THEYEAR}  % change date formatting


<<setup, eval= TRUE, include= FALSE, cache= FALSE, echo= FALSE>>=
system (paste ("biber", sub ("\\.Rnw$", "", current_input())))
@
\addbibresource{references.bib}

%========================================================================

\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{\fill}

		\LARGE 
		\textbf{Final Project Report}\\
		\vspace{0.5cm}
		\large STAT 230A -- Linear Models\\
		\vspace{0.5cm}

		\Large Huy G. Pham \& John V. Manousakis
		
		\vspace{0.5cm}
		\monthyeardate\today
		
		\vspace*{\fill}
	\end{center}
\end{titlepage}

\newpage

\tableofcontents

%========================================================================

%%%%%%%%%%%%%%%
% Conventions %
%%%%%%%%%%%%%%%

% These conventions can help us collectively develop a consistent report %

% Tense %
% - Let's stick to using a present tense instead of past
%   (i.e. we perform *this analysis*.. we demonstrate *that phenomenon*, ...)

% Use one line per sentence %
%  - works great with `$ git diff`
%  - it can help spot tedious long sentences
%  - we can add comments and just read those while skimming (for long narrative parts)
%    (this can also reveal inconsistencies in the logical flow of a paragraph)

% Commit early and often %

% library() %
% - load all libraries on the first chunk (to suppress output)
% - but if the code of a chunk makes use of a library, 
%   declare that in a comment. E.g.: # using library("car")
%   (easier for the reader to reuse the code)

%%%%%%%%%%%%%%%%%
% TEMPLATE AREA %
%%%%%%%%%%%%%%%%%

% things to copy-paste go here!


% chunk that is evaluated, shown and cached for faster compiling 

<<chunk_name, cache=TRUE>>=


@


% chunk that is evaluated but not shown (useful for package imports)

<<chunk_name2, echo = FALSE,message=FALSE, results='hide'>>=


@


% chunk that is displayed but not evaluated

<<chunk_name_3, eval=FALSE>>=


@

%========================================================================


<<echo = FALSE,message=FALSE>>=
library("car")
library("kableExtra")
library("nnet")
library("forcats")
library("afex")
library("AER")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% introduce dataset
We work on a dataset containing assessed values for residential properties sold in Ames, Iowa, in the late 2000s.
% why not use Boston Housing
Although the Boston housing dataset is commonly used, it no longer realistically represents today's housing market and contains fewer covariates.
Using the Ames dataset, inference on property values will better reflect today's reality, and there is more flexibility to demonstrate model selection techniques.
% where does it come from
The data set was compiled by prof. Dean DeCock and contains general information about a home's construction and features, such as size, condition, and year built, as well as the response variable: its sale price \parencite{DDC}.
% description
In general, information such as square footage of the house and its features provide continuous variables to perform linear analysis with;
in addition, the dataset contains various categorical features, such as roofing material or the presence/absence of amenities like garages or pools.
% the repository
All files associated with this analysis are publicly available on GitHub in the following URL:
\url{https://github.com/ioannis-vm/STAT230-Project\_Final}
The repository contains the code that generates this report, auxiliary files we used to develop it, and the dataset accompanied with a text file that describes its fields.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Import and Cleanup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following code imports the dataset and performs some cleanup operations.
% explain cleanup operations
Many variables had to be manually converted to factors.
The variables \texttt{Overall.Qual} and \texttt{Overall.Cond} are ratings on a scale from 1 to 10 of the overall quality and condition of the considered house, respectively.
We treat those variables as continuous since it is reasonable to assume that they are linearly related to the sale price.
We remove columns that contain NA values for more than 10\% of the observations.
We also remove the variables \texttt{Order} and \texttt{PID}, which are not predictor variables. 

\begin{singlespace}
<<import_cleanup, cache=TRUE>>=

# read in data
data.dir <- "../data/"
setwd(data.dir)
ames.full <- read.csv(file = 'AmesHousing.txt', sep = "\t")
# get dimensions
dims <- dim(ames.full)
# perform conversions
for (name in names(ames.full)) {
  if (typeof(ames.full[,name]) == "character"){
    ames.full[,name] <- as.factor(ames.full[,name])
  }
}
ames.full$Overall.Qual <- as.numeric(ames.full$Overall.Qual)
ames.full$Overall.Cond <- as.numeric(ames.full$Overall.Cond)
f <- function(col.name) {
  na.pct = sum(is.na(ames.full[col.name])) / dims[1]
  if (na.pct>0.10){
    return(TRUE)
  } else {
    return(FALSE)
  }
}
cols.to.remove <- names(ames.full)[sapply(names(ames.full), f)]
ames <- ames.full[, -which(names(ames.full) %in% cols.to.remove)]
ames <- ames[, -which(names(ames) %in% c("Order", "PID"))]
# drop the rest of the observations that contain Na's
ames <- ames[complete.cases(ames), ]

@
\end{singlespace}

The cleanup operations remove 8.4\% of the observations, as demonstrated with the following code.

\begin{singlespace}
<<cleanup_percent, cache=TRUE>>=

cat((1 - dim(ames)[1] / dim(ames.full)[1]) * 100, " %")

@
\end{singlespace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normal Linear Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro
This section demonstrates the use of the normal linear model for making inference about the regression coefficients and the predicted values obtained using the model.

\subsection{Univariate Regression}
We examine the effect of \texttt{Gr.Liv.Area} (the total living area of the house) on \texttt{SalePrice}.

\begin{singlespace}
<<univariate_fit, cache=TRUE>>=
fit <- lm(SalePrice ~ Gr.Liv.Area, data=ames)
summary(fit)
@
\end{singlespace}

We can see that the coefficient of \texttt{Gr.Liv.Area} and the intercept are non-zero with very high significance.

% diagnostic plot introduction
We generate four regression diagnostic plots.
% residuals vs. fitted
The Residuals vs. Fitted plot is a scatter plot of the residuals versus the predicted values.
It can be used to identify heteroskedasticity and assess the overall quality of the fit.
% normal q-q
The Normal Q-Q plot tests the normality assumption of the residuals by comparing the theoretical versus the observed quantile of each value.
% scale-location
The Scale-Location plot is another way to assess homoskedasticity.
The red line indicates the average magnitude of the residuals as a function of the fitted values.
A horizontal line implies homoskedasticity.
% residuals vs. leverage
Finally, the Residuals vs. Leverage helps identify outlier points.
Cook's distance can be used to classify points whose contribution to the coefficient estimates is very high.
On the plot, such points lie outside the boundary defined by the dashed red line.

\begin{singlespace}
<<univariate_fit_vis, cache=TRUE>>=
point.col = rgb(red=0.3, green=0.3, blue=0.3, alpha=0.4)  # point color for the plots
par(mfrow = c(2,2))
plot(fit, col = point.col)
@
\end{singlespace}

% outliers
We note that observations 1499, 2181, and 2182 are outliers.
Our model over-predicts the sale price of those houses.
% residuals: not normal dist
The Normal Q-Q plot reveals that the residuals are not normally distributed.
% heteroskedasticity
Finally, we can identify heteroskedasticity.
% conclusion
These observations make it clear that even though we can get an idea for the mean estimate of the sale price given the total area of the house, it would not be wise to make inference about the distribution of sale price conditioned on the total area by simply using a Gauss-Markov model.

\subsubsection{EHW Standard Error}

Identification of heteroskedasticity motivates the need to calculate the Eicker–Huber–White standard errors for this model.
The following code\footnote{Adapted from the course material.} computes the t-statistic for the intercept and the coefficient using the five corrections covered in the course material.
Lower t-statistic values correspond to higher standard errors.

\begin{singlespace}
<<univariate_fit_ehq, cache=TRUE>>=

# using library("car")
fit.hc0 = sqrt(diag(hccm(fit, type="hc0")))
fit.hc1 = sqrt(diag(hccm(fit, type="hc1")))
fit.hc2 = sqrt(diag(hccm(fit, type="hc2")))
fit.hc3 = sqrt(diag(hccm(fit, type="hc3")))
fit.hc4 = sqrt(diag(hccm(fit, type="hc4")))
fit.coef = summary(fit)$coef
tvalues = fit.coef [ ,1] /
cbind(fit.coef[ ,2] , fit.hc0 , fit.hc1 ,
fit.hc2 , fit.hc3 , fit.hc4)
colnames(tvalues) = c("ols",  "hc0", "hc1", "hc2", "hc3", "hc4")
round(tvalues , 2)

@
\end{singlespace}

We observe that all five EHW corrections result in increased standard errors for the regression coefficients, which is aligning with our expectation.

\subsection{Multivariate Regression}

This section demonstrates multivariate regression of the sale price conditioned on all other available information, using a Normal Linear Model.
% factors
Factor variables are accounted for via interaction coefficients that modify the intercept according to the level of the factor.
Due to the large number of factor variables, no other interaction coefficients associated with factor levels were included in the model (other than those for the intercept).
Modifying only the intercept leads to a model that has the geometric interpretation of parallel hyperplanes.
% outliers
Before we perform the regression, we perform some further data processing that essentially eliminates outliers and post-processes the dataset after their elimination.
\footnote{\texttt{Utilities} ended up containing a single level ("All public") and had to be removed.}


\begin{singlespace}
<<multivariate_regression_fit, cache=TRUE>>=
# initial fit
fit <- lm(SalePrice ~ ., data=ames)
# eliminate redundant covariates (do not add to the rank of X)
ames <- ames[, -which(names(ames) %in% c("Total.Bsmt.SF", "Gr.Liv.Area"))]
# eliminate outliers
ids <- which(  (abs(residuals(fit))>1.5e5) | (hatvalues(fit)>0.99) )
ames <- ames[-ids, ]
# remove single-level factor variables
ames <- droplevels(ames)
ames <- ames[, -which(names(ames) %in% c("Utilities"))]

# final fit
fit <- lm(SalePrice ~ ., data=ames)
@
\end{singlespace}

The summary of this model is very long, as it contains 213 coefficients (plus the intercept), for most of which we cannot reject the null hypothesis that their actual value is zero.
As with the univariate case, we make regression diagnostic plots.

<<multivariate_regression_summary, cache=TRUE>>=
point.col = rgb(red=0.3, green=0.3, blue=0.3, alpha=0.4)  # point color for the plots
par(mfrow = c(2,2))
plot(fit, col = point.col)
@

% diagnostic interpretation
The diagnostic plots show that this time homoskedasticity is reduced.
Non-Normality of the residuals is more pronounced (in particular, their distribution has a much thinner tail on both sides).
Finally, we no longer see outlier points with Cook's distance higher than 0.5.

% comments on that model
% we are overfitting! simplify the model
Even though this model better fits the data, it might not be the most suitable to use.
With 223 coefficients (70 variables, some of which are factors), this model has much flexibility to adapt to the observations, resulting in a lower mean squared error value than what could be obtained with simpler models.
However, this in no way guarantees that it better explains reality.
% example
For instance, it might be the case that some of the variability in the data is caused by external factors, for which we do not have any information in the dataset (which is not unreasonable to assume).
If that is the case, the augmented flexibility of our model will allow for the influence of those external factors to manifest itself as aliasing on the coefficients we are trying to predict.
This can make features appear significantly relevant when in reality, they are not.
% second example
In addition, depending on our use case, we might need a model that is easier to interpret.
% conclude & bridge: model selection
For all these reasons, we often need to simplify our models.
However, the simplifying process needs to be conducted using a consistent method and quantifiable metrics.
Cross-validation is computationally intensive, and the number of available features makes it infeasible to implement the method naively (we could, of course, hand-pick features and limit the options).
In the following section, we perform model selection with R's \texttt{step} function, which uses AIC to reveal the optimal number of features.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro
In this section, we use stepwise regression to determine the features that result in the model that has the lowest AIC.
% what is AIC
The Akaike Information Criterion (AIC) is a parametric quantity that can characterize models based on how well they fit the data while favoring models that involve fewer features.
$$
AIC = n \log \left( \frac{RSS}{n} \right)  + 2 p
$$
$n$ is the number of observations, $p$ is the number of features, and $RSS$ is the residual sum of squares.
Aiming to minimize RSS while maintaining a low $p$, we look for the combination of features that results in the lowest AIC.
% what stepwise does
Stepwise regression adds or removes one feature at a time (trying out all possible options) and commits to the change that causes the AIC score to drop the most.
% why not do an exhaustive search
With 70 possible covariates (not including the factor-level intercept interaction coefficients), we have practically infinite combinations of models.
$$2^{70} = 1,180,591,620,717,411,303,424$$
It is therefore infeasible to perform an exhaustive search, which motivates the use of stepwise regression.
However, stepwise regression can lead to a worse model than what we would have obtained from an exhaustive search due to the stepwise nature of feature addition or removal.

% huge output
The following code performs the analysis.\footnote{Due to the very long output produced by the \texttt{step} function, we suppress the output here and manually insert the results in the report.}

% chunk that is displayed but not evaluated
\begin{singlespace}
<<eval=FALSE>>=
# stepwise regression starting from the complete model
fit.sub <- step(fit, direction="both")
# stepwise regression starting from the 'mean' model
fit2 <- lm('SalePrice ~ 1', ames)
fit2.sub <- step(fit, direction="both")
@
\end{singlespace}

% chunk that is evaluated but not shown

<<stepwise, echo=FALSE, results='hide', cache=TRUE>>=
fit.sub <- step(fit, direction="both")
fit2 <- lm('SalePrice ~ 1', ames)
fit2.sub <- step(fit, direction="both")
@

The following figure shows the AIC obtained in each step starting from the full regression (and going backwards), as well as starting from the mean model (and going forwards).
While this is not always the case, here both directions result in the same model, with an AIC value of 52797.19 and 52 covariates.

% chunk that is evaluated but not shown
<<stepwise_plot, echo=FALSE, cache=TRUE, fig.width=6, fig.height=5>>=
aics1 <- c(
  52849.04,
  52838.28,
  52833.20,
  52828.37,
  52823.55,
  52818.69,
  52815.19,
  52812.08,
  52810.09,
  52808.31,
  52806.57,
  52804.81,
  52803.39,
  52802.19,
  52800.85,
  52799.57,
  52798.40,
  52797.62,
  52797.19
)
aics2 <- c(
  57186.83,
  56389.64,
  55652.23,
  55094.59,
  54770.15,
  54431.61,
  54215.40,
  53987.55,
  53852.11,
  53755.49,
  53643.97,
  53551.99,
  53473.09,
  53407.27,
  53348.52,
  53298.78,
  53258.22,
  53219.79,
  53182.73,
  53148.41,
  53118.05,
  53088.61,
  53059.70,
  53035.32,
  53009.37,
  52961.77,
  52937.91,
  52922.43,
  52910.93,
  52900.50,
  52891.29,
  52881.63,
  52872.36,
  52863.86,
  52855.11,
  52848.39,
  52839.48,
  52833.62,
  52828.62,
  52824.22,
  52819.76,
  52815.59,
  52811.57,
  52807.44,
  52803.82,
  52801.54,
  52800.02,
  52799.01,
  52798.12,
  52797.60,
  52797.24,
  52797.19
)



x1 <- c(70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52)
x2 <- c(1:52)
plot( x2, aics2, type = "b", col='#00008b',
      ylab="AIC", xlab="Step", main="AIC at each step", ylim=c(52000,58000), log = "y", xlim=c(0,70))
lines(x1, aics1, type = "b", col='#bd5d04')
abline(h = 52797.19, lty=2)
abline(v=52, lty=3)
legend("right", legend=c("Forward", "Backward"),
       col=c("#00008b", "#bd5d04"), lty=1, cex=0.8)

@

The plot reveals that starting from the complete model converges faster to the optimum since its AIC is already closer to that of the optimum model.
We also note that even though the optimum model in terms of AIC has 52 explanatory variables, an extensive range of simpler models has a similar AIC.

We can verify that the optimum models obtained from both directions are the same.

\begin{singlespace}
<<stepwise_final_compare, cache=TRUE>>=
p1 <- sort(names(fit2.sub$coefficients)) # get names from one direction
p2 <- sort(names(fit.sub$coefficients))  # get names from the other dir
bool <- c() # instantiate a container for the results
for (i in 1:length(p1)) {
  bool <- append(bool, p1[i] == p2[i]) # check
}
prod(bool) == 1 # assert all components are TRUE
@
\end{singlespace}

Finally, we present the summary of the optimum model.

\small
\begin{singlespace}
<<stepwise_final_summary, cache=TRUE>>=
summary(fit.sub)
@
\end{singlespace}


% TODO Perhaps the summary should be placed in an appendix.









\end{document}
