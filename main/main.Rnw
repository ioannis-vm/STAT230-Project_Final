\documentclass[11pt]{article}

\usepackage{comment,fancyhdr,amsthm,amsmath,hyperref,comment,hyperref,enumerate,float,datetime, microtype, setspace}
\usepackage[headheight=14.5pt, width=6.2in,top=1.1in,bottom=1.1in,bindingoffset=3mm]{geometry} % margins
\usepackage[backend=biber,style=authoryear,sorting=none, maxcitenames=2]{biblatex}

\pagestyle{fancy}  % add headers and footers

\renewcommand{\baselinestretch}{1.50}  %  set default line spacing
\newdateformat{monthyeardate}{\monthname[\THEMONTH], \THEYEAR}  % change date formatting

% commands for math
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

<<setup, eval= TRUE, include= FALSE, cache= FALSE, echo= FALSE>>=
system (paste ("biber", sub ("\\.Rnw$", "", current_input())))
@
\addbibresource{references.bib}

%========================================================================

\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{\fill}

		\LARGE 
		\textbf{Final Project Report}\\
		\vspace{0.5cm}
		\large STAT 230A -- Linear Models\\
		\vspace{0.5cm}

		\Large Huy G. Pham \& John V. Manousakis
		
		\vspace{0.5cm}
		\monthyeardate\today
		
		\vspace*{\fill}
	\end{center}
\end{titlepage}

\newpage

\tableofcontents

%========================================================================

%%%%%%%%%%%%%%%
% Conventions %
%%%%%%%%%%%%%%%

% These conventions can help us collectively develop a consistent report %

% Tense %
% - Let's stick to using a present tense instead of past
%   (i.e. we perform *this analysis*.. we demonstrate *that phenomenon*, ...)

% Use one line per sentence %
%  - works great with `$ git diff`
%  - it can help spot tedious long sentences
%  - we can add comments and just read those while skimming (for long narrative parts)
%    (this can also reveal inconsistencies in the logical flow of a paragraph)

% Commit early and often %

% library() %
% - load all libraries on the first chunk (to suppress output)
% - but if the code of a chunk makes use of a library, 
%   declare that in a comment. E.g.: # using library("car")
%   (easier for the reader to reuse the code)

%%%%%%%%%%%%%%%%%
% TEMPLATE AREA %
%%%%%%%%%%%%%%%%%

% things to copy-paste go here!


% chunk that is evaluated, shown and cached for faster compiling 

<<chunk_name, cache=TRUE>>=


@


% chunk that is evaluated but not shown (useful for package imports)

<<chunk_name2, echo = FALSE,message=FALSE, results='hide'>>=


@


% chunk that is displayed but not evaluated

<<chunk_name_3, eval=FALSE>>=


@

%========================================================================


<<echo = FALSE,message=FALSE>>=
library("car")
library("kableExtra")
library("nnet")
library("forcats")
library("afex")
library("AER")
library("boot")
@

\newpage
%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

% introduce dataset
We work on a dataset containing assessed values for residential properties sold in Ames, Iowa, in the late 2000s.
% why not use Boston Housing
Although the Boston housing dataset is commonly used, it no longer realistically represents today's housing market and contains fewer covariates.
Using the Ames dataset, inference on property values will better reflect today's reality, and there is more flexibility to demonstrate model selection techniques.
% where does it come from
The data set was compiled by Prof. Dean DeCock and contains general information about a home's construction and features, such as size, condition, and year built, as well as the response variable: its sale price \parencite{DDC}, and later on, its building type.
% description
In general, information such as square footage of the house and its features provide continuous variables to perform linear analysis with;
in addition, the dataset contains various categorical features, such as roofing material or the presence/absence of amenities like garages or pools.
% the repository
All files associated with this analysis are publicly available on GitHub in the following URL:
\url{https://github.com/ioannis-vm/STAT230-Project\_Final}
The repository contains the code that generates this report, auxiliary files we used to develop it, and the dataset accompanied with a text file that describes its fields.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Import and Cleanup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following code imports the dataset and performs some cleanup operations.
% explain cleanup operations
Many variables had to be manually converted to factors.
The variables \texttt{Overall.Qual} and \texttt{Overall.Cond} are ratings on a scale from 1 to 10 of the overall quality and condition of the considered house, respectively.
We treat those variables as continuous since it is reasonable to assume that they are linearly related to the sale price.
We remove columns that contain NA values for more than 10\% of the observations.
We also remove the variables \texttt{Order} and \texttt{PID}, which are not predictor variables. 

\begin{singlespace}
<<import_cleanup, cache=TRUE>>=

# read in data
data.dir <- "../data/"
setwd(data.dir)
ames.full <- read.csv(file = 'AmesHousing.txt', sep = "\t")
# get dimensions
dims <- dim(ames.full)
# perform conversions
for (name in names(ames.full)) {
  if (typeof(ames.full[,name]) == "character"){
    ames.full[,name] <- as.factor(ames.full[,name])
  }
}
ames.full$Overall.Qual <- as.numeric(ames.full$Overall.Qual)
ames.full$Overall.Cond <- as.numeric(ames.full$Overall.Cond)
f <- function(col.name) {
  na.pct = sum(is.na(ames.full[col.name])) / dims[1]
  if (na.pct>0.10){
    return(TRUE)
  } else {
    return(FALSE)
  }
}
cols.to.remove <- names(ames.full)[sapply(names(ames.full), f)]
ames <- ames.full[, -which(names(ames.full) %in% cols.to.remove)]
ames <- ames[, -which(names(ames) %in% c("Order", "PID"))]
# drop the rest of the observations that contain Na's
ames <- ames[complete.cases(ames), ]

@
\end{singlespace}

The cleanup operations remove 8.4\% of the observations, as demonstrated with the following code.

\begin{singlespace}
<<cleanup_percent, cache=TRUE>>=

cat((1 - dim(ames)[1] / dim(ames.full)[1]) * 100, " %")

@
\end{singlespace}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normal Linear Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro
This section demonstrates the use of the normal linear model for making inference about the regression coefficients and the predicted values obtained using the model.

\subsection{Univariate Regression}
We examine the effect of \texttt{Gr.Liv.Area} (the total living area of the house) on \texttt{SalePrice}.

\begin{singlespace}
<<univariate_fit, cache=TRUE>>=
fit <- lm(SalePrice ~ Gr.Liv.Area, data=ames)
summary(fit)
@
\end{singlespace}

We can see that the coefficient of \texttt{Gr.Liv.Area} and the intercept are non-zero with very high significance.

% diagnostic plot introduction
We generate four regression diagnostic plots.
% residuals vs. fitted
The Residuals vs. Fitted plot is a scatter plot of the residuals versus the predicted values.
It can be used to identify heteroskedasticity and assess the overall quality of the fit.
% normal q-q
The Normal Q-Q plot tests the normality assumption of the residuals by comparing the theoretical versus the observed quantile of each value.
% scale-location
The Scale-Location plot is another way to assess homoskedasticity.
The red line indicates the average magnitude of the residuals as a function of the fitted values.
A horizontal line implies homoskedasticity.
% residuals vs. leverage
Finally, the Residuals vs. Leverage helps identify outlier points.
Cook's distance can be used to classify points whose contribution to the coefficient estimates is very high.
On the plot, such points lie outside the boundary defined by the dashed red line.

\begin{singlespace}
<<univariate_fit_vis, cache=TRUE>>=
point.col = rgb(red=0.3, green=0.3, blue=0.3, alpha=0.4)  # point color for the plots
par(mfrow = c(2,2))
plot(fit, col = point.col)
@
\end{singlespace}

% outliers
We note that observations 1499, 2181, and 2182 are outliers.
Our model over-predicts the sale price of those houses.
% residuals: not normal dist
The Normal Q-Q plot reveals that the residuals are not normally distributed.
% heteroskedasticity
Finally, we can identify heteroskedasticity.
% conclusion
These observations make it clear that even though we can get an idea for the mean estimate of the sale price given the total area of the house, it would not be wise to make inference about the distribution of sale price conditioned on the total area by simply using a Gauss-Markov model.

\subsubsection{EHW Standard Error}

Identification of heteroskedasticity motivates the need to calculate the Eicker–Huber–White standard errors for this model.
The following code\footnote{Adapted from the course material.} computes the t-statistic for the intercept and the coefficient using the five corrections covered in the course material.
Lower t-statistic values correspond to higher standard errors.

\begin{singlespace}
<<univariate_fit_ehq, cache=TRUE>>=

# using library("car")
fit.hc0 = sqrt(diag(hccm(fit, type="hc0")))
fit.hc1 = sqrt(diag(hccm(fit, type="hc1")))
fit.hc2 = sqrt(diag(hccm(fit, type="hc2")))
fit.hc3 = sqrt(diag(hccm(fit, type="hc3")))
fit.hc4 = sqrt(diag(hccm(fit, type="hc4")))
fit.coef = summary(fit)$coef
tvalues = fit.coef [ ,1] /
cbind(fit.coef[ ,2] , fit.hc0 , fit.hc1 ,
fit.hc2 , fit.hc3 , fit.hc4)
colnames(tvalues) = c("ols",  "hc0", "hc1", "hc2", "hc3", "hc4")
round(tvalues , 2)

@
\end{singlespace}

We observe that all five EHW corrections result in increased standard errors for the regression coefficients, which is aligning with our expectation.

\subsubsection{Confidence Intervals}

We can get parametric confidence intervals for the intercept and \texttt{Gr.Liv.Area}.
We expect those to differ from a bootstrap estimate, given what we observed in the previous section regarding the standard errors.

We compute the parametric confidence intervals with the following line:

<<param_confint, cache=TRUE>>=
confint(fit)
@

Bootstrap confidence intervals can be obtained as follows:

<<bootstrap_ci, cache=TRUE>>=
# using library(boot)
func = function(data,idx){
coef(lm(SalePrice ~ Gr.Liv.Area, data=data[idx,]))
}
B = boot(ames,func,R=10000)

(boot.ci(B,index=1,type="perc"))
(boot.ci(B,index=2,type="perc"))
@

As we expected, the confidence intervals obtained from the two methods do not match.
We accept the latter result, as the bootstrap estimate has weaker assumptions.

\subsection{Multivariate Regression}

This section demonstrates multivariate regression of the sale price conditioned on all other available information, using a Normal Linear Model.
% factors
Factor variables are accounted for via interaction coefficients that modify the intercept according to the level of the factor.
Due to the large number of factor variables, no other interaction coefficients associated with factor levels were included in the model (other than those for the intercept).
Modifying only the intercept leads to a model that has the geometric interpretation of parallel hyperplanes.
% outliers
Before we perform the regression, we perform some further data processing that essentially eliminates outliers and post-processes the dataset after their elimination.
\footnote{\texttt{Utilities} ended up containing a single level ("All public") and had to be removed.}


\begin{singlespace}
<<multivariate_regression_fit, cache=TRUE>>=
# initial fit
fit <- lm(SalePrice ~ ., data=ames)
# eliminate redundant covariates (do not add to the rank of X)
ames <- ames[, -which(names(ames) %in% c("Total.Bsmt.SF", "Gr.Liv.Area"))]
# eliminate outliers
ids <- which(  (abs(residuals(fit))>1.5e5) | (hatvalues(fit)>0.99) )
ames <- ames[-ids, ]
# remove single-level factor variables
ames <- droplevels(ames)
ames <- ames[, -which(names(ames) %in% c("Utilities"))]

# final fit
fit <- lm(SalePrice ~ ., data=ames)
@
\end{singlespace}

The summary of this model is very long, as it contains 213 coefficients (plus the intercept), for most of which we cannot reject the null hypothesis that their actual value is zero.
As with the univariate case, we make regression diagnostic plots.

\begin{singlespace}
<<multivariate_regression_summary, cache=TRUE>>=
point.col = rgb(red=0.3, green=0.3, blue=0.3, alpha=0.4)  # point color for the plots
par(mfrow = c(2,2))
plot(fit, col = point.col)
@
\end{singlespace}

% diagnostic interpretation
The diagnostic plots show that this time homoskedasticity is reduced.
Non-Normality of the residuals is more pronounced (in particular, their distribution has a much thinner tail on both sides).
Finally, we no longer see outlier points with Cook's distance higher than 0.5.

% comments on that model
% we are overfitting! simplify the model
Even though this model better fits the data, it might not be the most suitable to use.
With 223 coefficients (70 variables, some of which are factors), this model has much flexibility to adapt to the observations, resulting in a lower mean squared error value than what could be obtained with simpler models.
However, this in no way guarantees that it better explains reality.
% example
For instance, it might be the case that some of the variability in the data is caused by external factors, for which we do not have any information in the dataset (which is not unreasonable to assume).
If that is the case, the augmented flexibility of our model will allow for the influence of those external factors to manifest itself as aliasing on the coefficients we are trying to predict.
This can make features appear significantly relevant when in reality, they are not.
% second example
In addition, depending on our use case, we might need a model that is easier to interpret.
% conclude & bridge: model selection
For all these reasons, we often need to simplify our models.
However, the simplifying process needs to be conducted using a consistent method and quantifiable metrics.
Cross-validation is computationally intensive, and the number of available features makes it infeasible to implement the method naively (we could, of course, hand-pick features and limit the options).
In section \ref{sec:modelselection} we perform model selection with R's \texttt{step} function, which uses AIC to reveal the optimal number of features.
Before that, we demonstrate the use of principal component analysis for outlier detection through dimensionality reduction.





\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principal Component Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we demonstrate the use of principal component analysis (PCA) to identify outliers.
One common goal when using PCA is to reduce the dimensionality of the available data, while maintaining a notion of "relevant distance" between the observations.
There might be many reasons to do this, but in this section we will focus on outlier detection.

The following code performs the analysis.
Comments have been added to explain the goal of each step.

\begin{singlespace}
<<pca_calculations, cache=TRUE>>=
# remove Nas and keep all numeric columns
ames.numeric <- na.omit(subset(ames.full, select=c(
  SalePrice,
  Gr.Liv.Area,
  Year.Built,
    Lot.Area, Overall.Qual, Overall.Cond, 
    Garage.Area, Total.Bsmt.SF
)))

# PCA #

# perform PCA, centering and scaling the data
ames.pca <- prcomp(ames.numeric, center = TRUE, scale. = TRUE)

# Outlier Detection #

# compute the distance from the origin given the first three PCs
r2 <- ames.pca$x[,1]^2 + ames.pca$x[,2]^2 + ames.pca$x[,3]^2
# get the indices of the points sorted in decreasing distance from the origin
r2 <- order(r2, decreasing=TRUE)
@
\end{singlespace}

We can visualize the projections of all the observations on the plane defined by the first two principal components.

\begin{singlespace}
<<pca_plot, cache=TRUE, fig.width=4, fig.height=4>>=
plot(ames.pca$x[,1], ames.pca$x[,2], asp=1, col=point.col)
points(ames.pca$x[r2[1:3],1], ames.pca$x[r2[1:3],2], col='red', pch=5)
@
\end{singlespace}

We can also get the indices of the outliers identified in red in the plot above.

\begin{singlespace}
<<pca_idx, cache=TRUE>>=
r2[1:3]
@
\end{singlespace}





\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Selection}
\label{sec:modelselection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro
In this section, we use stepwise regression to determine the features that result in the model that has the lowest AIC.
% what is AIC
The Akaike Information Criterion (AIC) is a parametric quantity that can characterize models based on how well they fit the data while favoring models that involve fewer features.
$$
AIC = n \log \left( \frac{RSS}{n} \right)  + 2 p
$$
$n$ is the number of observations, $p$ is the number of features, and $RSS$ is the residual sum of squares.
Aiming to minimize RSS while maintaining a low $p$, we look for the combination of features that results in the lowest AIC.
% what stepwise does
Stepwise regression adds or removes one feature at a time (trying out all possible options) and commits to the change that causes the AIC score to drop the most.
% why not do an exhaustive search
With 70 possible covariates (not including the factor-level intercept interaction coefficients), we have practically infinite combinations of models.
$$2^{70} = 1,180,591,620,717,411,303,424$$
It is therefore infeasible to perform an exhaustive search, which motivates the use of stepwise regression.
However, stepwise regression can lead to a worse model than what we would have obtained from an exhaustive search due to the stepwise nature of feature addition or removal.

% huge output
The following code performs the analysis.\footnote{Due to the very long output produced by the \texttt{step} function, we suppress the output here and manually insert the results in the report.}

% chunk that is displayed but not evaluated
\begin{singlespace}
<<eval=FALSE>>=
# stepwise regression starting from the complete model
fit.sub <- step(fit, direction="both")
# stepwise regression starting from the 'mean' model
fit2 <- lm('SalePrice ~ 1', ames)
fit2.sub <- step(fit, direction="both")
@
\end{singlespace}

% chunk that is evaluated but not shown

<<stepwise, echo=FALSE, results='hide', cache=TRUE>>=
fit.sub <- step(fit, direction="both")
fit2 <- lm('SalePrice ~ 1', ames)
fit2.sub <- step(fit, direction="both")
@

The following figure shows the AIC obtained in each step starting from the full regression (and going backwards), as well as starting from the mean model (and going forwards).
While this is not always the case, here both directions result in the same model, with an AIC value of 52797.19 and 52 covariates.

% chunk that is evaluated but not shown
<<stepwise_plot, echo=FALSE, cache=TRUE, fig.width=6, fig.height=5>>=
aics1 <- c(
  52849.04,
  52838.28,
  52833.20,
  52828.37,
  52823.55,
  52818.69,
  52815.19,
  52812.08,
  52810.09,
  52808.31,
  52806.57,
  52804.81,
  52803.39,
  52802.19,
  52800.85,
  52799.57,
  52798.40,
  52797.62,
  52797.19
)
aics2 <- c(
  57186.83,
  56389.64,
  55652.23,
  55094.59,
  54770.15,
  54431.61,
  54215.40,
  53987.55,
  53852.11,
  53755.49,
  53643.97,
  53551.99,
  53473.09,
  53407.27,
  53348.52,
  53298.78,
  53258.22,
  53219.79,
  53182.73,
  53148.41,
  53118.05,
  53088.61,
  53059.70,
  53035.32,
  53009.37,
  52961.77,
  52937.91,
  52922.43,
  52910.93,
  52900.50,
  52891.29,
  52881.63,
  52872.36,
  52863.86,
  52855.11,
  52848.39,
  52839.48,
  52833.62,
  52828.62,
  52824.22,
  52819.76,
  52815.59,
  52811.57,
  52807.44,
  52803.82,
  52801.54,
  52800.02,
  52799.01,
  52798.12,
  52797.60,
  52797.24,
  52797.19
)



x1 <- c(70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52)
x2 <- c(1:52)
plot( x2, aics2, type = "b", col='#00008b',
      ylab="AIC", xlab="Step", main="AIC at each step", ylim=c(52000,58000), log = "y", xlim=c(0,70))
lines(x1, aics1, type = "b", col='#bd5d04')
abline(h = 52797.19, lty=2)
abline(v=52, lty=3)
legend("right", legend=c("Forward", "Backward"),
       col=c("#00008b", "#bd5d04"), lty=1, cex=0.8)

@

The plot reveals that starting from the complete model converges faster to the optimum since its AIC is already closer to that of the optimum model.
We also note that even though the optimum model in terms of AIC has 52 explanatory variables, an extensive range of simpler models has a similar AIC.

We can verify that the optimum models obtained from both directions are the same.

\begin{singlespace}
<<stepwise_final_compare, cache=TRUE>>=
p1 <- sort(names(fit2.sub$coefficients)) # get names from one direction
p2 <- sort(names(fit.sub$coefficients))  # get names from the other dir
bool <- c() # instantiate a container for the results
for (i in 1:length(p1)) {
  bool <- append(bool, p1[i] == p2[i]) # check
}
prod(bool) == 1 # assert all components are TRUE
@
\end{singlespace}














Finally, the summary of the optimum model can be presented using the \texttt{summary} command.
The output can be found in appendix \ref{sec:model_sel_out}.


% chunk that is displayed but not evaluated
\begin{singlespace}
<<eval=FALSE>>=
summary(fit.sub)
@
\end{singlespace}






\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LASSO Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro
We now seek to refine the covariates chosen in the model using a regularized regression.
% motivation
In general, it is prudent to selectively reduce the number of covariates used in order to safeguard against overfitting.
This is typically done through a form of regularized regression, which penalizes the presence of large numbers of coefficients.
Although the choice of ridge regression is also available, we choose LASSO since it allows for weaker covariates to be dropped out.
% overview
LASSO regression is equivalent to solving the regularized regression problem
$$
\hat{\beta} (t) = \argmin_{b_j} RSS(b_j) + \lambda \sum_{j=1}^p | b_j |
$$
where $\lambda$ is a tuning parameter.
% crossvalidation
In order to find the best tuning parameter to use, we will split our data into two halves, used for testing and training.
Below, we use the package \texttt{glmnet} to perform cross validation.
A 10-fold cross validation is performed by default for a range of $\lambda$ values.
From there, we choose the $\lambda$ value that results in the smallest cross validation mean squared error.

% plots

Below, we plot the coefficients as $\lambda$ penalty term decreases.
We can see that as variables enter if we relax the $\lambda$ threshold, the most important ones enter first: \texttt{Street} (type of access), \texttt{Condtn.2} (proximity to certain road features), \texttt{Overall.Qual}, \texttt{Exterior.Cond}, and \texttt{Kitchen.Abvgr} (number of kitchens above grade.)

\begin{singlespace}
<<LASSO_cv, cache=TRUE>>=
library(glmnet)
library(dplyr)
library(plotmo)

set.seed(1)
ames.train <- ames %>% sample_frac(0.5)
ames.test <- ames %>% setdiff(ames.train)
x.train <- select(ames.train, -SalePrice) %>% data.matrix()
y.train <- ames.train$SalePrice
x.test <- select(ames.test, -SalePrice) %>% data.matrix()
y.test <- ames.test$SalePrice
lambdas <- 10^seq(10, -2, length = 100)

lasso.mod <- glmnet(x.train, 
                   y.train, 
                   alpha = 1, 
                   lambda = lambdas) # Fit lasso model on training data

plot_glmnet(lasso.mod, label=5)    # Draw plot of coefficients
@
\end{singlespace}

We also plot the progression of the mean squared error as the $\lambda$ value changes.
Note that eventually, there is a tradeoff between the $\lambda$ penalty and how much MSE is decreased.

\begin{singlespace}
<<LASSO_morePlots, cache=TRUE>>=

cv.out <- cv.glmnet(x.train, y.train, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam <- cv.out$lambda.min # Select lambda that minimizes training MSE
@
\end{singlespace}

We can now use the optimal $\lambda$ value in the LASSO prediction and output the mean-squared error from prediction.
Note that several coefficients are filtered out to be zero: \texttt{House.Style}, \texttt{Condtn.1} (redundant), \texttt{BsmtFin.Type.2} (Rating of basement finished area), \texttt{Half.Bath}, \texttt{Garage.Yr.Blt}, \texttt{Pool.Area}, and \texttt{Misc.Val} (value of miscellaneous features).

\begin{singlespace}
<<LASSO_regr, cache=TRUE>>=

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test) # Use best lambda to predict test data
lasso.mse <- mean((lasso.pred - y.test)^2) # Calculate test MSE
lasso.mse

x <- select(ames, -SalePrice) %>% data.matrix()
y <- ames$SalePrice

out <- glmnet(x, y, alpha = 1, lambda = lambdas) # Fit lasso model on full dataset
lasso.coef <- predict(out, type = "coefficients", s = bestlam) # Display coefficients using lambda chosen by CV
lasso.coef
@
\end{singlespace}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression for categorical outcome}
\label{sec:logistic_regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Intro
To further examine the data, we look at a case where we seek to predict the type of dwelling (e.g. one-family construction, duplex, townhouses). 
From the set of covariates provided, the predictions for the class of dwelling, along with its probability is outputted.
To do so, we use the multinomial logistic model for nominal outcomes, where we view the probability of a home being classified as a certain category as 
$$
\log \frac{\pi_k(x_i)}{\pi_K(x_i)} = x_i^T \beta_k
$$
which uses the logit odds model for probability.

% Data clarification
The dataset contains several columns that directly contain classifications that are directly related to the home type, such as \texttt{MS.SubClass} (nominal type of dwelling) and \texttt{House.Style} (nominal style of dwelling). 
These columns are omitted, and the regression is done to predict the factor levels for the category \texttt{Bldg.Type}, which is a nominal type of dwelling categorized into \texttt{1Fam} (single-family detached), \texttt{2FmCon} (two family conversions), \texttt{Duplx} (duplexes), and \texttt{Twnhse} (townhouse).
The original dataset lists two types of townhouses: end unit and inside unit.
These are condensed into one category.

% Logistic regression
From there, a generalized linear model is fit with the link function being the logit form. 

<<type_logistic, cache=TRUE>>=
library(nnet)
library(forcats)

ames$Bldg.Type <- fct_collapse(ames$Bldg.Type, Twnhs = c("Twnhs","TwnhsE"))

type.fit.area <- multinom(Bldg.Type ~  Lot.Area, data = ames)

# subclass defines the dwelling type, so let's leave it out
type.fit.all <- multinom(Bldg.Type ~  . - MS.SubClass - House.Style, data = ames)
type.fit.1 <- multinom(Bldg.Type ~  1, data = ames)

# Z test for significance
zTest <- summary(type.fit.all)$coefficients/summary(type.fit.all)$standard.errors
pTest <- (1 - pnorm(abs(zTest), 0, 1)) * 2
@

The predicted probabilities for the building type of the first 100 buildings is presented in appendix \ref{sec:logistic_reg_out}.
They can be obtained by issuing the following:

% chunk that is displayed but not evaluated
\begin{singlespace}
<<eval=FALSE>>=
predict(type.fit.all, newdata=ames, type = "prob")[1:100,]
@
\end{singlespace}

We can then take the highest probability as the predicted classification of the home.
Cross-checking this with the actual building type as recorded in the dataset, we arrive at a percentage of correct predictions.

Percentage of correct predictions using all covariates:

\begin{singlespace}
<<type_logistic_percentage, cache=TRUE>>=
predict.type <- predict(type.fit.all, newdata=ames, type = "class")
type.accuracy <- sum((predict.type == ames$Bldg.Type))/dim(ames)[1]*100
type.accuracy
@
\end{singlespace}

% Hypothesis testing
What if we're interested in reducing the covariates again?
Firstly, a hypothesis test for whether or not the coefficient for the covariate equals zero can be carried out.
This is simply a Wald test for the regression coefficient, and we can then output the p-value to quantify the test results.
From the p-values detailed below, we find that several covariates offer high p-values, meaning that they are likely to not contribute to the prediction (likely having $\beta = 0$).
Most notably, variables pertaining to the details of features such as a porch, garage, basement, or veneer are not significant to predicting the type of building. 

\begin{singlespace}
<<type_logistic_hyp, cache=TRUE>>=
# Z test for significance
zTest <- summary(type.fit.all)$coefficients/summary(type.fit.all)$standard.errors
pTest <- (1 - pnorm(abs(zTest), 0, 1)) * 2
@
\end{singlespace}

Next, we can try to perform an analysis of variance in order to find the least significant covariates.
To do so, we test the results of the ANOVA with a $\chi^2$ test and consider those with a low test-value to be non-significant variables.
Next, a prediction is performed with the covariates, excluding the non-predictors found with ANOVA (along with \texttt{MS.SubClass} and \texttt{House.Style}).
Again, we'll output the accuracy of the predictions cross-checked with the true building types as recorded.

\begin{singlespace}
<<type_logistic_anova, cache=TRUE>>=
# Z test for significance
# Likelihood ratio test for significance
LRTAnova <- Anova(type.fit.all)

# predictors whose coefficients are tested against 0
typeNonPredictors <- row.names(LRTAnova[LRTAnova$`Pr(>Chisq)`<=0.9,])
typeNonPredictors
typeNonPredictors <- c(typeNonPredictors, "MS.SubClass", "House.Style")

# short regression
ames.short <- ames[, !colnames(ames) %in% typeNonPredictors]
type.fit.short <- multinom(Bldg.Type ~ ., data=ames.short)
predict.short <- predict(type.fit.short, newdata=ames.short, type = "class")

short.accuracy <- sum((predict.short == ames.short$Bldg.Type))/dim(ames.short)[1]*100
short.accuracy
@
\end{singlespace}

We note that there is a drop in the prediction accuracy by two percentage points.
It is also of note that due to the high volume of single-family homes, the prediction accuracy might be skewed such that predicting every construction to be single-family results in a fairly high accuracy.

\section{Summary}


\newpage

\appendix

\section{Model Selection Output}
\label{sec:model_sel_out}

This is the output of the call \texttt{summary(fit.sub)} issued in section \ref{sec:modelselection}.

% chunk that is evaluated but not shown
<<stepwise_final_summary, echo=FALSE, cache=TRUE>>=
summary(fit.sub)
@




\section{Logistic Regression Output}
\label{sec:logistic_reg_out}

This is the output of the call \texttt{predict(type.fit.all, newdata=ames, type = "prob")[1:100,]} issued in section \ref{sec:logistic_regression}.

% chunk that is evaluated but not shown
<<logistic_predictions, echo=FALSE, cache=TRUE>>=
predict(type.fit.all, newdata=ames, type = "prob")[1:100,]
@


\end{document}
